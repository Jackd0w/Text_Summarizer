{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1696b9fb630>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA = False#torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:2\" if USE_CUDA else \"cpu\")\n",
    "SEED = 23\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose directory to save trained model data if training is interupted\n",
    "save_dir = os.path.join(\"preprocessed_data\", \"save\")\n",
    "# path to data files\n",
    "train_path = \"full_dataset_train.csv\"\n",
    "test_path = \"full_dataset_test.csv\"\n",
    "val_path = \"full_dataset_val.csv\"\n",
    "gensim_path = \"../lt2212-v19-a4/GoogleNews-vectors-negative300.bin\"\n",
    "pre_trained = False # this is not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teenager three elderly people saudi arabia haj...</td>\n",
       "      <td>virus kills teenager three elderly people on h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jury selection begins monday federal hate crim...</td>\n",
       "      <td>two men are accused of beating a mexican immig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kevin mccarthy whose 65 year long acting caree...</td>\n",
       "      <td>mccarthy died of natural causes in massachuset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>know saying best youve got learn best might tr...</td>\n",
       "      <td>ski fish play drums you can learn how at the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quitter proud november 20 37th anniversary gre...</td>\n",
       "      <td>november 20 marks the 37th great american smok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  \\\n",
       "0  teenager three elderly people saudi arabia haj...   \n",
       "1  jury selection begins monday federal hate crim...   \n",
       "2  kevin mccarthy whose 65 year long acting caree...   \n",
       "3  know saying best youve got learn best might tr...   \n",
       "4  quitter proud november 20 37th anniversary gre...   \n",
       "\n",
       "                                          highlights  \n",
       "0  virus kills teenager three elderly people on h...  \n",
       "1  two men are accused of beating a mexican immig...  \n",
       "2  mccarthy died of natural causes in massachuset...  \n",
       "3  ski fish play drums you can learn how at the f...  \n",
       "4  november 20 marks the 37th great american smok...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train_data = pd.read_csv(train_path) \n",
    "# Preview the first 5 lines of the loaded data \n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токены\n",
    "PAD_token = 0  # Padding\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "UNK_token = 3  # Unknown word token\n",
    "\n",
    "PAD_str = '<pad>' \n",
    "SOS_str = '[START]'\n",
    "EOS_str = '[END]'\n",
    "UNK_str = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.embeddings = {}        \n",
    "        self.index2word = {PAD_token: PAD_str, SOS_token: SOS_str, EOS_token: EOS_str, UNK_str: UNK_token}\n",
    "        self.num_words = 4  # Count SOS, EOS, PAD, UNK        \n",
    "        \n",
    "    \n",
    "    def addEmbedding(self,model,word): \n",
    "        vector = get_w2v_vectors(model, word)\n",
    "        self.embeddings[word] = vector\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: PAD_str, SOS_token: SOS_str, EOS_token: EOS_str, UNK_str: UNK_token}\n",
    "        self.num_words = 4 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "def load_gensim_model(path_to_model):\n",
    "    model =  KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "    return model\n",
    "\n",
    "def get_w2v_vectors(w2v_model, words):\n",
    "    w2v_vectors = {}\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec = w2v_model.word_vec(word)\n",
    "            w2v_vectors[word] = vec\n",
    "        # this exception will occur when a word does not exist in the vocabulary of this model\n",
    "        except KeyError:\n",
    "                vec = np.random.rand(1,300)[0]\n",
    "    return w2v_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Counting word pairs and adding embedding vectors.\n",
      "Counted words: 120792\n"
     ]
    }
   ],
   "source": [
    "# Using the functions defined above, return a populated vocab object and pairs list for the train data set\n",
    "def loadPrepareVocab(train_data):\n",
    "    import itertools as iter \n",
    "    print(\"Start preparing training data ...\")      \n",
    "    vocab = Vocab(\"CNN\")\n",
    "    t = train_data[\"story\"].tolist()\n",
    "    r = train_data[\"highlights\"].tolist()\n",
    "    # get the length of the longest sentence\n",
    "    max_length = len(max(t, key=len).split(' '))\n",
    "    # Split every line into pairs\n",
    "    pairs = list(zip(t,r))\n",
    "    if pre_trained:\n",
    "        print(\"Loading gensim model...\")\n",
    "        gensim = load_gensim_model(gensim_path)\n",
    "        print(\"Finished loading gensim model.\")\n",
    "    print(\"Counting word pairs and adding embedding vectors.\")\n",
    "    for pair in pairs:        \n",
    "        vocab.addSentence(pair[0])\n",
    "        vocab.addSentence(pair[1])    \n",
    "        if pre_trained:\n",
    "            vocab.addEmbedding(gensim,pair[0])\n",
    "            vocab.addEmbedding(gensim,pair[1])        \n",
    "    print(\"Counted words:\", vocab.num_words)\n",
    "    return vocab, pairs, max_length\n",
    "\n",
    "vocab, pairs, max_length = loadPrepareVocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_data = pd.read_csv(val_path) \n",
    "\n",
    "validation_data.head()\n",
    "\n",
    "def addVal(validation_data):\n",
    "    t = validation_data[\"story\"].tolist()\n",
    "    r = validation_data[\"highlights\"].tolist()\n",
    "    for words in list(zip(t,r)):     \n",
    "        vocab.addSentence(words[0])\n",
    "        vocab.addSentence(words[1]) \n",
    "addVal(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовка для валидации\n",
    "def prepareData(data):\n",
    "    print(\"Start preparing data ...\") \n",
    "    t = data[\"story\"].tolist()\n",
    "    r = data[\"highlights\"].tolist()\n",
    "    pairs = list(zip(t,r))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story\n",
      "teenager three elderly people saudi arabia hajj pilgrimage died h1n1 flu virus saudi health ministry said victims virus 17 year old nigerian female sudanese man indian man moroccan woman 75 years old cases discovered late said dr khaled al marghalani ministry spokesman old others pre existing chronic conditions al marghalani said sudanese man initially went doctor treated h1n1 doctor sent hospital treated h1n1 late said annual pilgrimage mecca saudi arabia required muslims least lives millions people around globe make trek attend year hajj begins wednesday many pilgrims arriving days weeks ahead event coincides flu season arab health ministers met cairo egypt months back looked like several groups might even banned years event flu children 12 adults 65 pregnant women people chronic illnesses saudis ban anybody coming left responsibility pilgrims countries origin effect officials issued guidelines people risk two days ago saudi arabias health minister dr abdullah al rabeeah gathered representatives discuss efforts detect ailment would nice time prepare [for possibility h1n1 spreading saudi arabia] ahead hajj satisfied measures taken great confidence said meeting al marghalani said safe weapon kinds h1n1 cases resulted deaths tamiflu drug used fight influenza lose tamiflu lose war tamiflu effective first 48 hours symptoms appear said\n",
      "Highlight\n",
      "virus kills teenager three elderly people on hajj pilgrimage arab ministers considered barring some groups from this years event saudis left responsibility to pilgrims countries of origin\n",
      "Story\n",
      "jury selection begins monday federal hate crime case two pennsylvania men accused fatally beating mexican immigrant shouting racial epithets white jury last year convicted brandon piekarsky derrick donchak misdemeanor simple assault acquitted felony counts including aggravated assault ethnic intimidation hindering apprehension 2008 beating luis ramirez aftermath divided small rural mining town shenandoah pennsylvania along racial lines drew national attention verdict pennsylvania gov ed rendell sent letter us attorney general eric holder recommending justice department pursue civil rights charges evidence suggests mr ramirez targeted beaten killed mexican rendell wrote lawlessness violence hurts victim attack also towns communities torn apart bigotry intolerance jurors found piekarsky guilty third degree murder prosecutors alleged delivered fatal kick ramirezs head ramirez knocked ground alcohol fueled brawl residential shenandoah street last years trial two sentenced 23 months county jail convicted hate crime charges donchak piekarsky face maximum penalty life prison donchak also faces maximum 20 years prison convicted obstruction additional five years conspiring obstruct justice violence motivated bigotry hate place america yet remains prevalent many communities said assistant attorney general thomas perez justice department statement announcing indictment last year jury selection finished federal case trial us district court scranton pennsylvania expected take five days according information posted courts website months attorneys haggling details federal trial motion filed august lawyers representing piekarsky donchak criticized proposed jury question list federal prosecutors submitted claiming question list included plans question jurors registration vote political philosophy attitude toward enforcing law books read bumper stickers car government attempted politicize case beginning proposed questionnaire attempt motion said continued premise governments questions political philosophy obvious government deems white rural conservatives likely bigots unable apply law follow courts instructions case premise insulting fails courts obligation ensure fair impartial jury final question list approved us district judge richard caputo sealed last year pennsylvania prosecutors alleged group teens including donchak piekarsky baited ramirez undocumented mexican immigrant confrontation following night drinking donchak convicted corrupting minors providing alcohol friends fight according federal indictment donchak piekarsky walking home local festival encountered ramirez attacked street striking kicking members group yelled racial slurs justice department said medical examiner ruled ramirez died blunt force trauma head according indictment\n",
      "Highlight\n",
      "two men are accused of beating a mexican immigrant following a night of drinking if convicted they could face life in prison the case has divided a rural mining town and drawn national attention attorneys for the defendants say the government is trying to politicize the case\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pair in pairs[:2]:\n",
    "    print(\"Story\")\n",
    "    print(pair[0])\n",
    "    print(\"Highlight\")\n",
    "    print(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    sentence_indices = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            sentence_indices.append(lang.word2index[word])\n",
    "        except KeyError:\n",
    "                sentence_indices.append(UNK_token)\n",
    "    return sentence_indices + [EOS_token]\n",
    "\n",
    "def embeddingsFromSentence(lang, sentence):\n",
    "    sentence_embeddings = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            sentence_embeddings.append(lang.embeddings[word])\n",
    "        except KeyError:\n",
    "                vec = np.random.rand(1,300)[0]\n",
    "    return sentence_embeddings\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, vocab):\n",
    "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, vocab):\n",
    "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(vocab, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, vocab)\n",
    "    output, mask, max_target_len = outputVar(output_batch, vocab)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[  469,  1054],\n",
      "        [  932,  2312],\n",
      "        [ 1539,  1216],\n",
      "        [  260,   785],\n",
      "        [  217,   945],\n",
      "        [ 1812,  4358],\n",
      "        [34389, 12698],\n",
      "        [11547, 12699],\n",
      "        [33483,  3827],\n",
      "        [ 8013,  3113],\n",
      "        [33475,  2579],\n",
      "        [10314,   470],\n",
      "        [ 2234,   651],\n",
      "        [  391,   652],\n",
      "        [ 1754,  1174],\n",
      "        [11507,   651],\n",
      "        [ 6531,  1965],\n",
      "        [   18,  9207],\n",
      "        [  423,  3622],\n",
      "        [11547, 15495],\n",
      "        [ 1577,  1174],\n",
      "        [  409,  2587],\n",
      "        [ 1889,  4930],\n",
      "        [ 4661, 12700],\n",
      "        [ 4714,  2579],\n",
      "        [14639,  1054],\n",
      "        [ 8671,  2312],\n",
      "        [34390,  1216],\n",
      "        [   99,   705],\n",
      "        [ 3245,  5898],\n",
      "        [ 4981,  1699],\n",
      "        [ 1447,  1971],\n",
      "        [ 5102,  1207],\n",
      "        [ 4502,   122],\n",
      "        [  289,  1502],\n",
      "        [ 3469,  1498],\n",
      "        [ 3431, 12700],\n",
      "        [  806,  1656],\n",
      "        [  294,     2],\n",
      "        [ 1990,     0],\n",
      "        [ 6531,     0],\n",
      "        [   50,     0],\n",
      "        [   18,     0],\n",
      "        [ 2754,     0],\n",
      "        [ 2942,     0],\n",
      "        [ 1598,     0],\n",
      "        [ 1447,     0],\n",
      "        [ 1040,     0],\n",
      "        [ 4502,     0],\n",
      "        [ 1571,     0],\n",
      "        [ 7633,     0],\n",
      "        [   21,     0],\n",
      "        [   22,     0],\n",
      "        [ 2089,     0],\n",
      "        [10053,     0],\n",
      "        [  459,     0],\n",
      "        [   50,     0],\n",
      "        [  423,     0],\n",
      "        [  303,     0],\n",
      "        [  391,     0],\n",
      "        [ 1754,     0],\n",
      "        [  108,     0],\n",
      "        [   19,     0],\n",
      "        [  816,     0],\n",
      "        [ 3431,     0],\n",
      "        [   50,     0],\n",
      "        [   40,     0],\n",
      "        [ 4502,     0],\n",
      "        [   48,     0],\n",
      "        [  459,     0],\n",
      "        [ 1996,     0],\n",
      "        [  932,     0],\n",
      "        [11547,     0],\n",
      "        [ 3574,     0],\n",
      "        [ 1372,     0],\n",
      "        [ 6601,     0],\n",
      "        [   29,     0],\n",
      "        [12605,     0],\n",
      "        [ 4502,     0],\n",
      "        [12967,     0],\n",
      "        [   46,     0],\n",
      "        [ 5380,     0],\n",
      "        [  833,     0],\n",
      "        [   80,     0],\n",
      "        [ 5380,     0],\n",
      "        [ 3433,     0],\n",
      "        [  423,     0],\n",
      "        [  946,     0],\n",
      "        [ 7418,     0],\n",
      "        [ 8013,     0],\n",
      "        [10314,     0],\n",
      "        [ 5371,     0],\n",
      "        [  895,     0],\n",
      "        [33475,     0],\n",
      "        [10314,     0],\n",
      "        [50127,     0],\n",
      "        [ 4237,     0],\n",
      "        [10913,     0],\n",
      "        [11547,     0],\n",
      "        [33481,     0],\n",
      "        [ 5695,     0],\n",
      "        [ 8612,     0],\n",
      "        [10621,     0],\n",
      "        [  147,     0],\n",
      "        [  122,     0],\n",
      "        [  799,     0],\n",
      "        [  147,     0],\n",
      "        [ 4196,     0],\n",
      "        [ 4714,     0],\n",
      "        [  391,     0],\n",
      "        [ 1754,     0],\n",
      "        [   18,     0],\n",
      "        [15025,     0],\n",
      "        [10314,     0],\n",
      "        [  241,     0],\n",
      "        [10315,     0],\n",
      "        [ 1996,     0],\n",
      "        [  423,     0],\n",
      "        [11547,     0],\n",
      "        [   60,     0],\n",
      "        [ 2192,     0],\n",
      "        [  504,     0],\n",
      "        [ 3049,     0],\n",
      "        [11944,     0],\n",
      "        [   50,     0],\n",
      "        [   18,     0],\n",
      "        [ 6282,     0],\n",
      "        [ 9646,     0],\n",
      "        [ 1649,     0],\n",
      "        [13320,     0],\n",
      "        [ 1562,     0],\n",
      "        [ 4176,     0],\n",
      "        [ 2606,     0],\n",
      "        [ 3302,     0],\n",
      "        [  833,     0],\n",
      "        [ 3389,     0],\n",
      "        [ 1532,     0],\n",
      "        [  409,     0],\n",
      "        [ 9094,     0],\n",
      "        [ 4553,     0],\n",
      "        [34390,     0],\n",
      "        [ 5542,     0],\n",
      "        [   21,     0],\n",
      "        [   22,     0],\n",
      "        [11507,     0],\n",
      "        [ 3057,     0],\n",
      "        [  234,     0],\n",
      "        [   83,     0],\n",
      "        [   40,     0],\n",
      "        [17817,     0],\n",
      "        [10079,     0],\n",
      "        [  817,     0],\n",
      "        [11507,     0],\n",
      "        [  411,     0],\n",
      "        [39199,     0],\n",
      "        [11547,     0],\n",
      "        [ 2065,     0],\n",
      "        [17176,     0],\n",
      "        [  133,     0],\n",
      "        [  331,     0],\n",
      "        [ 2360,     0],\n",
      "        [ 3857,     0],\n",
      "        [   18,     0],\n",
      "        [  423,     0],\n",
      "        [34390,     0],\n",
      "        [ 4449,     0],\n",
      "        [ 8233,     0],\n",
      "        [50128,     0],\n",
      "        [ 4717,     0],\n",
      "        [ 1642,     0],\n",
      "        [ 2166,     0],\n",
      "        [  590,     0],\n",
      "        [ 2873,     0],\n",
      "        [ 1789,     0],\n",
      "        [ 6787,     0],\n",
      "        [  130,     0],\n",
      "        [ 1649,     0],\n",
      "        [ 5853,     0],\n",
      "        [  938,     0],\n",
      "        [47565,     0],\n",
      "        [  453,     0],\n",
      "        [34390,     0],\n",
      "        [ 3582,     0],\n",
      "        [45351,     0],\n",
      "        [  991,     0],\n",
      "        [  149,     0],\n",
      "        [ 4714,     0],\n",
      "        [10901,     0],\n",
      "        [17817,     0],\n",
      "        [ 4714,     0],\n",
      "        [50129,     0],\n",
      "        [20782,     0],\n",
      "        [  423,     0],\n",
      "        [  946,     0],\n",
      "        [  147,     0],\n",
      "        [  122,     0],\n",
      "        [  799,     0],\n",
      "        [ 1571,     0],\n",
      "        [  991,     0],\n",
      "        [  104,     0],\n",
      "        [  549,     0],\n",
      "        [  574,     0],\n",
      "        [ 1905,     0],\n",
      "        [ 1324,     0],\n",
      "        [14406,     0],\n",
      "        [  991,     0],\n",
      "        [ 5723,     0],\n",
      "        [  904,     0],\n",
      "        [24525,     0],\n",
      "        [ 4292,     0],\n",
      "        [  657,     0],\n",
      "        [38805,     0],\n",
      "        [   89,     0],\n",
      "        [ 1307,     0],\n",
      "        [ 1308,     0],\n",
      "        [ 2984,     0],\n",
      "        [12032,     0],\n",
      "        [17867,     0],\n",
      "        [50130,     0],\n",
      "        [36084,     0],\n",
      "        [ 4806,     0],\n",
      "        [  881,     0],\n",
      "        [ 7713,     0],\n",
      "        [ 3728,     0],\n",
      "        [  469,     0],\n",
      "        [   29,     0],\n",
      "        [50131,     0],\n",
      "        [ 1447,     0],\n",
      "        [ 6667,     0],\n",
      "        [17817,     0],\n",
      "        [  991,     0],\n",
      "        [29794,     0],\n",
      "        [38772,     0],\n",
      "        [  338,     0],\n",
      "        [  286,     0],\n",
      "        [14584,     0],\n",
      "        [22976,     0],\n",
      "        [15370,     0],\n",
      "        [ 9071,     0],\n",
      "        [ 3225,     0],\n",
      "        [ 4918,     0],\n",
      "        [ 1827,     0],\n",
      "        [14264,     0],\n",
      "        [ 7713,     0],\n",
      "        [ 4270,     0],\n",
      "        [ 1628,     0],\n",
      "        [   19,     0],\n",
      "        [17817,     0],\n",
      "        [  445,     0],\n",
      "        [14133,     0],\n",
      "        [20332,     0],\n",
      "        [   18,     0],\n",
      "        [ 5029,     0],\n",
      "        [  991,     0],\n",
      "        [ 2212,     0],\n",
      "        [15894,     0],\n",
      "        [   18,     0],\n",
      "        [   35,     0],\n",
      "        [ 9055,     0],\n",
      "        [50132,     0],\n",
      "        [ 8875,     0],\n",
      "        [ 1544,     0],\n",
      "        [38979,     0],\n",
      "        [   18,     0],\n",
      "        [ 1536,     0],\n",
      "        [11547,     0],\n",
      "        [  340,     0],\n",
      "        [ 7841,     0],\n",
      "        [ 1114,     0],\n",
      "        [ 2441,     0],\n",
      "        [ 1689,     0],\n",
      "        [ 6863,     0],\n",
      "        [11547,     0],\n",
      "        [ 2143,     0],\n",
      "        [ 3653,     0],\n",
      "        [   18,     0],\n",
      "        [  401,     0],\n",
      "        [  994,     0],\n",
      "        [  531,     0],\n",
      "        [ 3235,     0],\n",
      "        [ 2455,     0],\n",
      "        [17646,     0],\n",
      "        [ 2940,     0],\n",
      "        [ 1536,     0],\n",
      "        [ 4441,     0],\n",
      "        [ 2676,     0],\n",
      "        [11547,     0],\n",
      "        [ 4728,     0],\n",
      "        [ 2329,     0],\n",
      "        [ 7506,     0],\n",
      "        [ 4862,     0],\n",
      "        [ 7581,     0],\n",
      "        [ 6604,     0],\n",
      "        [  131,     0],\n",
      "        [ 4456,     0],\n",
      "        [ 1504,     0],\n",
      "        [50133,     0],\n",
      "        [  527,     0],\n",
      "        [  707,     0],\n",
      "        [   18,     0],\n",
      "        [  234,     0],\n",
      "        [  179,     0],\n",
      "        [ 2049,     0],\n",
      "        [  240,     0],\n",
      "        [  322,     0],\n",
      "        [ 2020,     0],\n",
      "        [   21,     0],\n",
      "        [   22,     0],\n",
      "        [ 2511,     0],\n",
      "        [ 5893,     0],\n",
      "        [ 6667,     0],\n",
      "        [16005,     0],\n",
      "        [ 1544,     0],\n",
      "        [  164,     0],\n",
      "        [  364,     0],\n",
      "        [  974,     0],\n",
      "        [ 5853,     0],\n",
      "        [50134,     0],\n",
      "        [17954,     0],\n",
      "        [11547,     0],\n",
      "        [26756,     0],\n",
      "        [ 1565,     0],\n",
      "        [ 1593,     0],\n",
      "        [  240,     0],\n",
      "        [  343,     0],\n",
      "        [ 1577,     0],\n",
      "        [ 1536,     0],\n",
      "        [  459,     0],\n",
      "        [ 5058,     0],\n",
      "        [ 4624,     0],\n",
      "        [50135,     0],\n",
      "        [ 1689,     0],\n",
      "        [  149,     0],\n",
      "        [ 4714,     0],\n",
      "        [  707,     0],\n",
      "        [ 1996,     0],\n",
      "        [  249,     0],\n",
      "        [ 2357,     0],\n",
      "        [ 1013,     0],\n",
      "        [ 9070,     0],\n",
      "        [34390,     0],\n",
      "        [11167,     0],\n",
      "        [ 1075,     0],\n",
      "        [14715,     0],\n",
      "        [  141,     0],\n",
      "        [11499,     0],\n",
      "        [ 2201,     0],\n",
      "        [ 6828,     0],\n",
      "        [ 2357,     0],\n",
      "        [ 3493,     0],\n",
      "        [ 2177,     0],\n",
      "        [ 7850,     0],\n",
      "        [50128,     0],\n",
      "        [14715,     0],\n",
      "        [  401,     0],\n",
      "        [  302,     0],\n",
      "        [  657,     0],\n",
      "        [   68,     0],\n",
      "        [ 1059,     0],\n",
      "        [ 5047,     0],\n",
      "        [  303,     0],\n",
      "        [50136,     0],\n",
      "        [  271,     0],\n",
      "        [ 5059,     0],\n",
      "        [  224,     0],\n",
      "        [ 5806,     0],\n",
      "        [ 5991,     0],\n",
      "        [50137,     0],\n",
      "        [    2,     0]])\n",
      "lengths: tensor([369,  39])\n",
      "target_variable: tensor([[  651,   403],\n",
      "        [11547,  2346],\n",
      "        [  410, 12700],\n",
      "        [ 2946,   410],\n",
      "        [  403,   398],\n",
      "        [33483,  3557],\n",
      "        [  405,  4404],\n",
      "        [ 8013,   160],\n",
      "        [  153,  7884],\n",
      "        [ 2947,    68],\n",
      "        [ 1052,  1054],\n",
      "        [  403,  2312],\n",
      "        [   50,  1216],\n",
      "        [  730,   705],\n",
      "        [  651,  2145],\n",
      "        [    5,   158],\n",
      "        [    7, 12700],\n",
      "        [ 4642,   159],\n",
      "        [  402,   496],\n",
      "        [ 4714,   906],\n",
      "        [  397,   766],\n",
      "        [  806,  3225],\n",
      "        [ 1570, 25404],\n",
      "        [ 1278,   405],\n",
      "        [  403, 19763],\n",
      "        [ 1290,  2145],\n",
      "        [  160,   403],\n",
      "        [ 2089,  2589],\n",
      "        [10053, 27075],\n",
      "        [  403,   159],\n",
      "        [15732,  4048],\n",
      "        [  991,  1051],\n",
      "        [  904,  6527],\n",
      "        [  160,   160],\n",
      "        [ 2594,  7919],\n",
      "        [  403,   761],\n",
      "        [ 4714,   927],\n",
      "        [ 1252,   153],\n",
      "        [20782,  1054],\n",
      "        [  153,  2312],\n",
      "        [  423,  1216],\n",
      "        [    2,     2]])\n",
      "mask: tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "max_target_len: 42\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 2\n",
    "batches = batch2TrainData(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because the input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=max_length):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "    \n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(vocab, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training model...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'vocab_dict': vocab.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the model is saved as\n",
    "model_name = 'CNN_model_2'\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "# Choose if using pre-trained Glove embeddings or not\n",
    "pre_trained = False # this is not implemented\n",
    "# Configure model paramters\n",
    "attn_model = 'dot'\n",
    "hidden_size = 300\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Finished building models.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_iter = 4\n",
    "\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    vocab.__dict__ = checkpoint['vocab_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "embedding = nn.Embedding(vocab.num_words, hidden_size)\n",
    "\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Finished building models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training/optimization paramteters\n",
    "clip = 10.0\n",
    "teacher_forcing_ratio = 3.0\n",
    "learning_rate = 0.01\n",
    "decoder_learning_ratio = 1.5\n",
    "n_iteration = 20\n",
    "print_every = 1\n",
    "save_every = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Finished building optimizers.\n"
     ]
    }
   ],
   "source": [
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "            \n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "print('Finished building optimizers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Initializing ...\n",
      "Training model...\n",
      "Iteration: 1; Percent complete: 5.0%; Average loss: nan\n",
      "Iteration: 2; Percent complete: 10.0%; Average loss: nan\n",
      "Iteration: 3; Percent complete: 15.0%; Average loss: nan\n",
      "Iteration: 4; Percent complete: 20.0%; Average loss: nan\n",
      "Iteration: 5; Percent complete: 25.0%; Average loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/3684737978.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run training iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting Training...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m train_model(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[0m\u001b[0;32m      4\u001b[0m            \u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m            print_every, save_every, clip, 'CNN', loadFilename)\n",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/2072041859.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Run a training iteration with batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0m\u001b[0;32m     23\u001b[0m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/2180147643.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_target_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             decoder_output, decoder_hidden = decoder(\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\LaughingMan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/54969897.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_step, last_hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Predict next word using Luong eq. 6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Return output and final hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\LaughingMan\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1512\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1513\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1514\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training iterations\n",
    "print(\"Starting Training...\")\n",
    "train_model(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, 'CNN', loadFilename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, vocab, sentence, max_length=max_length):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(vocab, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [vocab.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateInput(input_sentence, encoder, decoder, searcher, vocab):\n",
    "    # Evaluate sentence\n",
    "    output_words = evaluate(encoder, decoder, searcher, vocab, input_sentence)\n",
    "    # Format and print response sentence\n",
    "    output_words[:] = [x for x in output_words if not (x == EOS_str or x == PAD_str)]\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us patient infection mysterious mers virus rec...</td>\n",
       "      <td>a patient with mers in florida has been discha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>digital era finding one often involves finding...</td>\n",
       "      <td>siren is a new dating app created for women by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dna explore unknown pushing boundaries explori...</td>\n",
       "      <td>the spaceshiptwo catastrophe comes after an or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nun died setting fire southwestern china first...</td>\n",
       "      <td>group tenzin wangmo called for religious freed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>point life wizened age 69 managed run 123 mara...</td>\n",
       "      <td>training is key when preparing for your first ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  \\\n",
       "0  us patient infection mysterious mers virus rec...   \n",
       "1  digital era finding one often involves finding...   \n",
       "2  dna explore unknown pushing boundaries explori...   \n",
       "3  nun died setting fire southwestern china first...   \n",
       "4  point life wizened age 69 managed run 123 mara...   \n",
       "\n",
       "                                          highlights  \n",
       "0  a patient with mers in florida has been discha...  \n",
       "1  siren is a new dating app created for women by...  \n",
       "2  the spaceshiptwo catastrophe comes after an or...  \n",
       "3  group tenzin wangmo called for religious freed...  \n",
       "4  training is key when preparing for your first ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "test_data = pd.read_csv(test_path) \n",
    "# Preview the first 5 lines of the loaded data \n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "story, summary = test_data[\"story\"].tolist(), test_data[\"highlights\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story:  us patient infection mysterious mers virus recovered florida department health announced monday patient health care provider lives works saudi arabia discharged department said admitted dr p phillips hospital orlando may 9 health care workers household contacts contact patient tested mers cov results come back negative health department said statement broad risk mers cov infection general public threat traveling orlando area mers cov stands middle east respiratory syndrome coronavirus florida patient us citizen said dr anne schuchat director cdcs national center immunization respiratory diseases one three people united states confirmed mers fatal first us diagnosis indiana man traveled saudi arabia also health care provider florida indiana cases linked schuchat said indiana patient extended face face contact shook hands man illinois 40 minute business meeting centers disease control prevention said saturday believed represent first transmission mers within united states officials said illinois man said suffering mild cold like symptoms seek require medical care dr david swerdlow told reporters blood test indicated previously infected mers think patient likely infected mers technically count official case mers said mers 5 things know mers date 570 confirmed cases mers including 171 deaths according world health organization number countries confirmed cases expanded 18 case netherlands according global health authority seen sharp uptick since middle march especially saudi arabia united arab emirates mers coronavirus group viruses common cold attacks respiratory system according cdc symptoms lead pneumonia kidney failure first found arabian peninsula 2012 one knows exactly virus originated evidence implicating camels emerging recently published study mbio researchers said isolated live mers virus two single humped camels known dromedaries found multiple substrains camel viruses including one perfectly matches substrain isolated human patient vaccine special treatment mers doctors say believe indiana patients quick diagnosis care dramatically increased chances getting better risk general public remains low schuchat said countries virus spread person person close contact person caring ill person virus shown ability spread easily person person community settings said abundance caution cdc contacting people passengers flights florida indiana patients confirmed mers schuchat said cases mers diagnosed result transmission plane cdcs dr marty cetron said florida patient 44 year old florida patient traveled may 1 jeddah saudi arabia london london boston boston atlanta finally atlanta orlando man began feeling unwell flight jeddah symptoms including fever chills slight cough schuchat said tests negative virus florida health department said opinion mers become global threat\n",
      "Summary:  a patient with mers in florida has been discharged three people in the us have had confirmed mers infections mers was first found in the arabian peninsula in 2012\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  digital era finding one often involves finding one dating app best suited individuals lifestyle research shows society becomes increasingly plugged attitudes shifted positively toward finding love online web dating longer associated recluses hiding behind computer screen fact one 10 americans used online dating site app according 2013 pew research report online relationships shift online dating branched beyond algorithm based matches highly specific sites focus particular niche even farmers dating site wanted part time lover enter siren dating app created women women puts ladies drivers seat comes shopping online connection women control sees image communicate type date pursue ceo susie lee design director katrina hess created siren based core principle women needed control visibility lee said meaning users give clear signals actually interested comfortable talking suitor choice online dating options grown lee noticed friends frustration every good introduction often came slew lewd ones started looking online dating options quickly realized many things immediately creepy meter went lee said subject come head users particularly women started publicizing complaints unsolicited sexual advances online 2013 pew research online dating report found 28 online daters contacted way made feel harassed uncomfortable 42 female online daters experiencing compared 17 men year comedian cracked writer alli reed created worlds worst dating profile okcupid see terrible attractive woman would stop indecent introductions okcupid set bad dates experiment snippets like really good convincing people pregnant parents think law school pay bills alongside models photograph still managed get fair share boorish messages artist anna gensler went far draw artistic nude interpretations color openers received tinder sent back sender tinder picks next match lee says app meant counterpart man woman haters club instead place women control images whether high profile position simply ward objectification lee likens visibility functionality real world interaction woman makes eye contact smiles signal interest potential suitor suitor ends untoward toggle visibility way could decline conversation social setting reason necessarily needed power men needed feel safe fun lee said heres works user signs siren prompted take app photo ensure really look like profile lee said next user prompted open ended question day female users see men responded day woman likes mans take either choose make visible save profile scope future responses get broader sense personality trying make strangers less strange order put something lee said adding hopes gives complete date personality portrait user typical profile list likes dislikes flip side male user see female users answers without seeing\n",
      "Summary:  siren is a new dating app created for women by women the founders say the idea came about after friends complaints of lewd online exchanges similar apps like hinge wyldfire and willow rely on other users vouching for potential suitors\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  dna explore unknown pushing boundaries exploring space far easy week commercial space industry received punch gut world experienced jarring reminder dangerous space travel 55th test flight virgin galactics spaceshiptwo suffered anomaly two minutes separated mother ship whiteknighttwo 45000 feet mojave desert one pilot able parachute treated serious injuries tragically perished catastrophe comes heels explosion happened earlier week unmanned rocket built orbital sciences carrying spacecraft loaded 5000 pounds cargo exploded fireball seconds launch nasas wallops flight facility virginia aerospace insiders routinely compare emerging commercial space industry genesis aviation industry high risk high reward virgin galactic founder richard branson said blog post following fridays incident always known road space extremely difficult every new transportation system deal bad days early history new private space race emerged recent years private companies like virgin galactic xcor aerospace competing become worlds first commercial space line companies like spacex orbital sciences boeing vying nasa contracts worth billions weeks events put spotlight private sectors ability shoulder responsibility space travel long term impacts disasters burgeoning commercial space industry noted terms larger government contracts relationship public private really new since inception space agency nasa turned private sector accomplish space faring goals boeing lead contractor international space station relationship see change anytime soon even face orbital sciences accident us longer home grown way get international space station nasa relying private sector get making commercial companies essential space agencys operations thus shielding impact public perception space tourism industry different service providing luxury experience companies mercy public perception capabilities safely transport paying passengers suborbital flights yes virgin partnered nasa run research missions zero gravity beyond virgin galactic relies selling tickets space enthusiasts adrenaline junkies bulk development cost cheap costs 250000 seat passengers want know going safe fridays accident surely instil fear 700 people signed make journey value space tourism risk human lives make reality george whitesides told cnn earlier year heart inspires idea space changes space profound impact people experience whitesides referring overview effect phenomenon space travelers said experience see curvature earth changing way people see world thereby influencing way live whitesides branson spoken openly fact suborbital flights ultimate end goal virgin galactic point point intercontinental travel would next application technology meaning one day passengers could travel around world two hours plans put hold since accident virgin galactic intent taking blog post following crash branson said space hard worth persevere move forward together question people still willing pay 250000 go space inside virgin\n",
      "Summary:  the spaceshiptwo catastrophe comes after an orbital sciences rocket blew up virgin galactic founder richard branson we have always known that the road to space is extremely difficult accidents have put spotlight on private sectors ability to shoulder responsibility of space travel virgin has spoken openly about its ambitions beyond suborbital flights\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  nun died setting fire southwestern china first tibetan woman known killed way london based free tibet campaign group said tuesday according group tenzin wangmo 20 called religious freedom tibet return dalai lama set fire outside dechen chokorling nunnery ngaba county sichuan province monday died scene state administration religious affairs beijing told cnn aware incident free tibet advocates tibetan independence also reported two tibetans shot wounded sunday security forces protest outside police station prefecture ganzi tenzin ninth tibetan monks former monks commit self immolation protest chinese rule since march advocacy group said five died last attempt took place saturday 19 year old former monk kirti monastery aba another ethnic tibetan area sichuan province set alight central market ngaba town survived whereabouts unknown according free tibet activists say disturbing acts reflect increasingly repressive environment beijings control tibetans live fear suppressive unfair government policies dare speak prominent tibetan writer activist tsering woeser told cnn tibetan buddhists cannot use violence protest therefore violence self immolation make people pay attention situation suicide sacrifice order draw worlds attention another incident aba march monk died sparked weeks often violent confrontation local monks authorities security forces locked kirti monastery eventually detained 300 monks rights groups said time united nations working group enforced disappearances expressed concern beijing chinese officials called organizations perspective biased unfair relevant local authorities conducting legal education kirti monastery monks maintain religious order question forced disappearances hong lei foreign ministry spokesman said june china also rejects accusations oppression tibetans saying rule greatly improved living standards tibetan people dalai lamas representative signed agreement beijing 1951 affirm chinas sovereignty tibet also grant autonomy area failed uprising beijings rule 1959 forced dalai lama exile dalai lama denies seeking independence tibet saying wants genuine autonomy tibetans make policies key issues religious practices 2008 uprising violent unrest tibet subsequent military crackdown left least 18 dead activists say tensions remained high many areas since\n",
      "Summary:  group tenzin wangmo called for religious freedom in tibet before setting herself alight tenzin is the ninth tibetan and first woman to commit self immolation free tibet also reported that two tibetans were shot and wounded during a protest activists acts reflect an increasingly repressive environment under beijings control\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  point life wizened age 69 managed run 123 marathons happy say still going strong things could turned differently fact first marathon almost last 1981 pretty strong runner never really attempted kind distance friend mine challenged give detroit marathon try young cocky pretty stupid trained sporadically went way fast start race spent last 10 miles feeling like going die time finished vowed never run another marathon promptly ran second one following year could resist challenge overcoming mistakes conviction could better lucky aside frustrated overwhelmed easily could injured would put end running completely course problems started training properly serious running first marathon training key million good training schedules point youve probably seen would like talk race ensure success finish feel reasonably good good enough back five things learned since first miserable experience 1 pacing learned first day pacing critical start slower think need go initial adrenaline rush make want go sprint drain rest race bonus good way figure healthy pace first marathon run 10k least month advance take 10k time multiply five give pretty accurate predictor marathon time run 55 minute 10k shoot 435 marathon 275 minutes 2 walking mixing walking running great way get marathon still feel strong water station try walking 30 60 seconds usually water station mile conserve energy finish time actually improve 3 talking know sounds crazy talking runners race really helpful keeping mind pain important engaging conversation great way breath talk running fast listening music another good way stay focused experience far less interesting 4 stretching stretching warming race really important preventing injury many good stretching guides available internet also highly recommend yoga course cannot yoga race try fit one two hours week training positions like warrior pose dog increase strength flexibility entire core running marathon legs whole body 5 experimenting sounds like common sense know many runners talked get brilliant idea try something completely different day marathon new shoes socks drinks energy gels big race stick rituals gear know trust save new stuff something easy like 5k happy running stick see boston\n",
      "Summary:  training is key when preparing for your first marathon runner says mixing walking and running is a good way to stay strong to the end john farah recommends multiplying your 10k pace by 5 to get your goal time\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  facebook says recently hacked though says data billion users compromised company described sophisticated attack blog post friday saying took place january small number employees visited compromised website installed malware machines soon discovered presence malware remediated infected machines informed law enforcement began significant investigation continues day facebook security said post facebook largest social network world latest high profile site hacked year twitter announced similar intrusion earlier month major news organizations including new york times wall street journal washington post also admitted hacked news sites attributed breaches hackers working chinese government neither facebook twitter mention china describing attacks facebook alone attack clear others attacked infiltrated recently well said blog post one first companies discover malware immediately took steps start sharing details infiltration companies entities affected unlike twitter facebook said found evidence user information compromised twitter said user names encrypted passwords e mail addresses many 250000 users potentially grabbed hackers reset passwords affected accounts string hacks primarily exploited vulnerabilities programming language java installed computers default facebook said site responsible attack took advantage previously unknown java vulnerability oracle patched february 1 january department homeland security issued alert security challenged software recommended people turn computers apple turned java default os x users precaution full instructions disable java computer found oracles website must use java make sure downloaded latest updates include key security patches facebook said continue work law enforcement others industry prevent future attacks\n",
      "Summary:  facebook says it was hacked in january when employees visited a compromised website the social network has found no evidence that any user data was obtained by the hackers this is latest in a string of high profile hacks this year\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  undocumented student mexico arrested held deportation facility stopped minor traffic violation georgia found guilty thursday driving without license jury cobb county georgia also found 22 year old jessica colotl guilty impeding traffic reason police gave pulling campus kennesaw state university conviction misdemeanor charge driving without license punishable year jail said lawyer jerome lee conviction includes mandatory minimum 48 hours jail colotl already served around 45 1 2 lee said lee said colotl plans appeal verdict colotl became lightning rod immigration debate last spring advocates immigration reform said case symbol broken system thursday lee said recently obtained learners permit said grounds dismiss misdemeanor charge georgia law jury essentially refused properly read statute juries often said colotls legal problems started late march car stopped campus kennesaw state born mexico living united states since 11 could produce drivers license handed identification expired passport mexico arrested next day turned immigration officials spent month etowah detention center alabama friends came force marched campus defense may released deportation deferred year allow finish studies quickly arrested warrant cobb county sheriffs office released 2500 bail trying live american dream finish education said time thursday lee said completing college clients plans future unclear many variables terms going happen said little early say\n",
      "Summary:  22 year old jessica colotl became a lighting rod in immigration debate this year the conviction carries maximum of 12 months in jail her lawyer says colotl will appeal the decision\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  new york giant lobster named george escaped dinner table fate released saturday atlantic ocean new york seafood restaurant granted freedom according statement people ethical treatment animals george lobster sort mascot city crab seafood new york lobster peta said 140 years old weighed 20 pounds confined tank city crab seafood restaurant manhattan two customers alerted animal group peta statement say extraordinary age estimate determined restaurant manager keith valenti told cnn lobsters grow pound every seven 10 years put georges weight 18 20 pounds 12 years biggest lobster ever seen valenti said said lobster sitting restaurants tank acting sort mascot peta got involved requested release seemed like right thing peta president ingrid newkirk said statement applaud folks city crab seafood compassionate decision allow noble old timer live days freedom peace hope kind gesture serves example intriguing animals deserve confined tiny tanks boiled alive shedding tight confines old restaurant display tank george driven maine peta members returned natural habitat ocean floor saturday organization said\n",
      "Summary:  peta estimated lobsters age at 140 years he weighed 20 pounds groups members drove him to maine and released him into the ocean restaurant manager said george was biggest lobster i have ever seen\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n",
      "Story:  republican candidates president outlined foreign policy national security visions debate saturday tackling issues largely taken backseat presidential race shaped troubled economy moderators pressed candidates range topics 90 minute debate south carolina including terrorism irans nuclear capabilities china war afghanistan called arab spring foreign aid debate returned several times question whether united states military intelligence personnel allowed waterboard suspected terrorists tactic banned president barack obama shortly took office 2009 candidates split matter texas gov rick perry seeking regain footing embarrassing mental lapse debate earlier week michigan forcefully defended practice war perry said happens war former pizza executive herman cain said waterboarding see torture see enhanced interrogation technique minnesota rep michele bachmann called tactic effective said obama allowing aclu run cia former utah gov jon huntsman former diplomat positioned candidate race serious foreign policy experience disagreed libertarian leaning texas rep ron paul cnn truth squad fact checking gop debate diminish standing world values project include liberty democracy human rights open markets torture huntsman said torture waterboarding torture dilute like whole lot countries lose ability project values lot people corners world still relying united states stand former massachusetts gov mitt romney address matter debate held wofford college spartanburg sponsored south carolina republican party cbs news national journal romney aides said forum believe waterboarding torture adviser eric fehrnstrom would say whether practice would used romney administration saying decisions enhanced interrogation president going spell would employ fehrnstrom told cnn romney embraced several hawkish positions said military forces able target kill american citizens fighting alongside enemies overseas even without trial pakistan also sights several candidates five things learned saturdays gop debate perry harshly accused pakistani government intelligence services knowingly harboring terrorists said united states stop sending billions dollars aid money country went saying financial aid every country zeroed evaluated would go israel perry said though clarified israel remains staunch american ally would continue receive funding substantial level former house speaker newt gingrich criticized pakistan harsh terms agreed perrys proposal vastly reduce amount federal money allocated foreign governments ought start zero say explain give penny gingrich said former pennsylvania sen rick santorum though said administration needs engage pakistan nuclear weapons could fall wrong hands different regime cannot indecisive whether pakistan friend santorum said must friend must engage friends\n",
      "Summary:  cain and bachmann support waterboarding suspected terrorists candidates face questions over iran arab spring perry seeks to regain his footing after an embarrassing mental lapse this week\n",
      "---------------------------------------------------------------\n",
      "Prediction:  \n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/1805148536.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluateInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Story: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/1459108719.py\u001b[0m in \u001b[0;36mevaluateInput\u001b[1;34m(input_sentence, encoder, decoder, searcher, vocab)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluateInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Evaluate sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0moutput_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Format and print response sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moutput_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_words\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEOS_str\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPAD_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/2742760242.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(encoder, decoder, searcher, vocab, sentence, max_length)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Decode sentence with searcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# indexes -> words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdecoded_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\LaughingMan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/313859794.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_seq, input_length, max_length)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Forward pass through decoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;31m# Obtain most likely word token and its softmax score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mdecoder_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\LaughingMan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LAUGHI~1\\AppData\\Local\\Temp/ipykernel_17872/54969897.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_step, last_hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mconcat_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Predict next word using Luong eq. 6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Return output and final hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\LaughingMan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\LaughingMan\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\LaughingMan\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for st,su in zip(story,summary):\n",
    "    pr = evaluateInput(st,encoder, decoder, searcher, vocab)\n",
    "    \n",
    "\n",
    "    print(\"Story: \", st)\n",
    "    print(\"Summary: \", su)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"Prediction: \", pr)\n",
    "    print(\"---------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e034dcd2142dcd32dab96a2e754d5e8e7b145fb5047b580a4287b09c1391a90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
