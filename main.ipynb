{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LaughingMan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from os import listdir\n",
    "import re\n",
    "import argparse\n",
    "import csv\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"cnn/stories\"\n",
    "save_file = \"full_dataset\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stories_maxlen = 500\n",
    "highlights_maxlen = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \"\"\" Загружаем данные в переменную \"\"\"\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def split_story(doc):\n",
    "    \"\"\" Делим файл на саму статью и заголовок \"\"\"\n",
    "    index = doc.find('@highlight')\n",
    "    story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "    highlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "    return story, highlights\n",
    "\n",
    "def load_stories(directory):\n",
    "    \"\"\" Сохраняем данные в директорию \"\"\"\n",
    "    stories = []\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        doc = load_doc(filename)\n",
    "        story, highlights = split_story(doc)\n",
    "        stories.append({'story': story, 'highlights': highlights})\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Обрабатываем сленг \"\"\"\n",
    "def change_contractions(word):\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\"\n",
    "    }\n",
    "    if word in contractions.keys():\n",
    "        word = contractions[word]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(lines):\n",
    "    \"\"\"Ищем CNN \"\"\"\n",
    "    cleaned = \"\"\n",
    "    for line in lines:\n",
    "        index = line.find('(CNN) -- ')\n",
    "        if index > -1:\n",
    "            line = line[index+len('(CNN)'):]\n",
    "        # remove CNN titles\n",
    "        line = line.replace('(CNN)', '')\n",
    "        if \"contributed to this report\" in line:\n",
    "            line = \" \"\n",
    "\n",
    "        # replace - and / with space to avoid compund words\n",
    "        line = line.replace('-', ' ').replace('/', ' ')\n",
    "        # remove wierd characters\n",
    "        line = re.sub(r'[\\?\\!\\\"\\*\\&\\:\\.\\,\\(\\)\\$\\;\\»\\Ã\\©\\Ã\\±\\@\\#\\%\\•\\+]', '', line)\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # replace word contractions will full words\n",
    "        line = [change_contractions(word) for word in line]\n",
    "        # remove empty lines\n",
    "        line = [c for c in line if len(c) > 0]\n",
    "        line = \" \".join(line)\n",
    "        # remove aphostrophes (after changing contractions)\n",
    "        line = re.sub(r'(\\')', '', line)\n",
    "        # remove more than 2 whitespaces\n",
    "        line = re.sub(r'[ ]{2,}', ' ', line)\n",
    "        if not len(line) == 0:\n",
    "            cleaned = cleaned+line+\" . \"\n",
    "\n",
    "    cleaned = cleaned[:-3]\n",
    "    #cleaned = re.sub(r'( \\. )', ' <e> <s> ', cleaned)\n",
    "    cleaned = re.sub(r'( \\. )', ' ', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_stories(cleaned, max_len):\n",
    "    cleaned = cleaned.split(\" \")\n",
    "    '''Удаляем все стоп-слова'''\n",
    "    cleaned = [c for c in cleaned if not c in stop_words]\n",
    "    cleaned = cleaned[0:max_len]\n",
    "    cleaned = \" \".join(cleaned)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(stories, size=0.20):\n",
    "    # Splitting data into sets\n",
    "    trainset, testset = train_test_split(stories, test_size=size)\n",
    "    # Printing the lengths of the sets\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Отделяем заголовки'''\n",
    "def cut_highlights(cleaned, max_len):\n",
    "    cleaned = cleaned.split(\" \")\n",
    "    cleaned = cleaned[0:max_len]\n",
    "    cleaned = \" \".join(cleaned)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Отчищаем раздел с историями'''\n",
    "def clean_stories(data_path, save_file):\n",
    "    print(\"Loading stories...\")\n",
    "    directory = data_path\n",
    "    stories = load_stories(directory)\n",
    "    print(\"Loaded number of stories: {}.\".format(len(stories)))\n",
    "    \n",
    "    # clean stories\n",
    "    for example in stories:\n",
    "        example[\"story\"] = clean_lines(example['story'].split('\\n'))\n",
    "        example[\"highlights\"] = clean_lines(example[\"highlights\"])\n",
    "        example[\"story\"] = cut_stories(example['story'], 400)\n",
    "        example[\"highlights\"] = cut_highlights(example[\"highlights\"], 100)\n",
    "        if example[\"story\"] == \"\":\n",
    "            example[\"story\"] = np.nan # way of dropping empty lines from pd object later\n",
    "\n",
    "    # split text\n",
    "    print(\"Splitting stories into train and test...\")\n",
    "    train, test = split_data(stories)\n",
    "    train, val = split_data(train)\n",
    "    print(\"Number of stories in training set: {}\".format(len(train)))\n",
    "    print(\"Number of stories in validation set: {}\".format(len(val)))\n",
    "    print(\"Number of stories in test set: {}\".format(len(test)))\n",
    "\n",
    "    print(\"Writing data to files...\")\n",
    "    test_filename = save_file + \"_train.csv\"\n",
    "    val_filename = save_file + \"_val.csv\"\n",
    "    train_filename = save_file + \"_test.csv\"\n",
    "\n",
    "    df1 = pd.DataFrame.from_dict(train)\n",
    "    df2 = pd.DataFrame.from_dict(test)\n",
    "    df3 = pd.DataFrame.from_dict(val) \n",
    "    \n",
    "    # Drop rows with any empty cells\n",
    "    df1.dropna(how='any', inplace=True)  \n",
    "    df2.dropna(how='any', inplace=True)   \n",
    "    df3.dropna(how='any', inplace=True)\n",
    "\n",
    "    # write to file\n",
    "    df3.to_csv(val_filename, encoding='utf-8', index=False)\n",
    "    df1.to_csv(train_filename, encoding='utf-8', index=False)\n",
    "    df2.to_csv(test_filename, encoding='utf-8', index=False)\n",
    "\n",
    "    print(\"Finished processing data, saved train, validation and test files as {}, {} and {} \".format(\n",
    "        train_filename, val_filename, test_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stories...\n",
      "Loaded number of stories: 92579.\n",
      "Splitting stories into train and test...\n",
      "Number of stories in training set: 59250\n",
      "Number of stories in validation set: 14813\n",
      "Number of stories in test set: 18516\n",
      "Writing data to files...\n",
      "Finished processing data, saved train, validation and test files as full_dataset_test.csv, full_dataset_val.csv and full_dataset_train.csv \n"
     ]
    }
   ],
   "source": [
    "clean_stories(data_path, save_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df009659529e0ec04777c337a67e583675df0f6e46840be81ae36cc04261b86c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('house_predict')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
